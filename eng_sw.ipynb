{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contexto:\n",
    "Como Analista de Fraude de uma empresa de seguros de carro, preciso de uma solução eficiente que utilize machine learning para detectar fraudes em grandes volumes de dados, a fim de melhorar a precisão na identificação de atividades fraudulentas e minimizar perdas financeiras para a empresa.\n",
    "\n",
    "Descrição: Como Analista de Fraude, Eu quero um aplicativo em Python que utilize técnicas de machine learning para analisar dados de seguros de carro, Para que eu possa detectar possíveis fraudes de maneira eficiente e eficaz.\n",
    "\n",
    "# Critérios de Aceitação:\n",
    "# Importação e Preparação de Dados:\n",
    "O aplicativo deve permitir a importação de dados de seguros a partir de arquivos CSV, Excel ou de um banco de dados.\n",
    "Deve realizar a limpeza dos dados, incluindo o tratamento de valores nulos e inconsistências.\n",
    "Deve realizar a transformação dos dados categóricos em formatos numéricos utilizáveis pelo modelo de machine learning.\n",
    "# Treinamento do Modelo:\n",
    "O aplicativo deve oferecer a opção de treinar um modelo de machine learning utilizando um dataset histórico de seguros previamente identificado como legítimo ou fraudulento.\n",
    "Deve incluir a divisão do dataset em conjuntos de treinamento e teste, e permitir a escolha do algoritmo de machine learning (e.g., Random Forest, Gradient Boosting, CatBoost).\n",
    "# Detecção de Fraude:\n",
    "O aplicativo deve ser capaz de aplicar o modelo treinado para prever a probabilidade de fraude em novos dados de seguros.\n",
    "Deve gerar uma lista de registros com suas respectivas probabilidades de serem fraudulentos, destacando os casos com maior risco.\n",
    "# Relatórios e Visualizações:\n",
    "O aplicativo deve fornecer relatórios detalhados com métricas de performance do modelo (e.g., precisão, recall, F1-score, AUC-ROC).\n",
    "Deve oferecer visualizações interativas, como gráficos de distribuição de fraudes detectadas, importância das features no modelo e comparações entre registros fraudulentos e legítimos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler,RobustScaler,Normalizer\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,roc_auc_score\n",
    "from typing import Union\n",
    "from sklearn.base import BaseEstimator\n",
    "from typing import Any, List\n",
    "import time\n",
    "from sklearn.base import BaseEstimator\n",
    "from typing import Any, List\n",
    "import time\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.model_selection._search import ParameterSampler\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "from sklearn.base import clone\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"\"\"\n",
    "    A class representing a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pandas.DataFrame): The input dataframe.\n",
    "    - cat_columns (list): A list of categorical column names.\n",
    "    - date_columns (list): A list of date column names.\n",
    "    - cont_columns (list): A list of continuous column names.\n",
    "    - disc_column (list): A list of discrete column names.\n",
    "    - label_column (str or list, optional): The label column name(s). Defaults to None.\n",
    "\n",
    "    Attributes:\n",
    "    - _df (pandas.DataFrame): The input dataframe.\n",
    "    - cat_columns (list): A list of categorical column names.\n",
    "    - date_columns (list): A list of date column names.\n",
    "    - cont_columns (list): A list of continuous column names.\n",
    "    - disc_columns (list): A list of discrete column names.\n",
    "    - labeled (bool): Indicates if the dataset is labeled.\n",
    "    - label_column (str or list): The label column name(s).\n",
    "    \n",
    "    Methods:\n",
    "    - check_columns(all_columns, *columns_groups): Checks if the given columns match the dataset columns.\n",
    "    - not_label_columns(date=False): Returns a list of column names excluding the label column(s).\n",
    "    - set_sample(sample, type, name, subtype=None): Sets a sample for a specific type and name.\n",
    "    - get_sample(type, name, subtype=None): Retrieves a sample for a specific type and name.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df, cat_columns, date_columns, cont_columns, disc_column, label_column=None):\n",
    "        self._df = df\n",
    "        self.cat_columns = cat_columns\n",
    "        self.date_columns = date_columns\n",
    "        self.cont_columns = cont_columns\n",
    "        self.disc_columns = disc_column\n",
    "        \n",
    "        if isinstance(label_column, str):\n",
    "            label_column = [label_column]\n",
    "            \n",
    "        if label_column is not None:\n",
    "            self.labeled = True\n",
    "            self.label_column = label_column\n",
    "        else:\n",
    "            self.labeled = False\n",
    "            self.label_column = []\n",
    "        \n",
    "    def check_columns(self, all_columns, *columns_groups):\n",
    "        \"\"\"\n",
    "        Checks if the given columns match the dataset columns.\n",
    "\n",
    "        Parameters:\n",
    "        - all_columns (list): A list of all column names.\n",
    "        - *columns_groups (list): Variable number of lists containing column names.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If the columns do not match.\n",
    "\n",
    "        \"\"\"\n",
    "        columns = []\n",
    "        for group in columns_groups:\n",
    "            if group:\n",
    "                columns += group\n",
    "        if len(all_columns) == len(columns):\n",
    "            raise ValueError(f\"Columns do not match: {set(all_columns).difference(columns)}\")        \n",
    "        \n",
    "    @property\n",
    "    def not_label_columns(self, date=False):\n",
    "        \"\"\"\n",
    "        Returns a list of column names excluding the label column(s).\n",
    "\n",
    "        Parameters:\n",
    "        - date (bool, optional): Indicates if date columns should be included. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "        - list: A list of column names.\n",
    "\n",
    "        \"\"\"\n",
    "        if not date:\n",
    "            return self.cat_columns + self.cont_columns + self.disc_columns\n",
    "        else:\n",
    "            return self.cat_columns + self.date_columns + self.cont_columns + self.disc_columns\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        \"\"\"\n",
    "        Returns the input dataframe.\n",
    "\n",
    "        Returns:\n",
    "        - pandas.DataFrame: The input dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "        return self._df\n",
    "    \n",
    "    @df.setter\n",
    "    def df(self, df):\n",
    "        \"\"\"\n",
    "        Sets the input dataframe.\n",
    "\n",
    "        Parameters:\n",
    "        - df (pandas.DataFrame): The input dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "        self._df = df\n",
    "\n",
    "    def set_sample(self, sample, type, name, subtype=None):\n",
    "        \"\"\"\n",
    "        Sets a sample for a specific type and name.\n",
    "\n",
    "        Parameters:\n",
    "        - sample: The sample to be set.\n",
    "        - type (str): The type of the sample (train, validation, or test).\n",
    "        - name (str): The name of the sample (X or Y).\n",
    "        - subtype (str, optional): The subtype of the sample (scaled or None). Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If the type, name, or subtype is invalid.\n",
    "\n",
    "        \"\"\"\n",
    "        if type not in [\"train\", \"validation\", \"test\"]:\n",
    "            raise ValueError(\"Type must be train, validation, or test\")\n",
    "        if name not in [\"X\", \"Y\"]:\n",
    "            raise ValueError(\"Name must be X or Y\")\n",
    "        if subtype not in [\"scaled\", None]:\n",
    "            raise ValueError(\"Subtype must be scaled or none\")\n",
    "        \n",
    "        att_name = name + \"_\" + type if subtype is None else name + \"_\" + type + \"_\" + subtype\n",
    "\n",
    "        setattr(self, att_name, sample)\n",
    "    \n",
    "    def get_sample(self, type, name, subtype=None):\n",
    "        \"\"\"\n",
    "        Retrieves a sample for a specific type and name.\n",
    "\n",
    "        Parameters:\n",
    "        - type (str): The type of the sample (train, validation, or test).\n",
    "        - name (str): The name of the sample (X or Y).\n",
    "        - subtype (str, optional): The subtype of the sample (scaled or None). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "        - The requested sample.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If the type, name, or subtype is invalid.\n",
    "\n",
    "        \"\"\"\n",
    "        if type not in [\"train\", \"validation\", \"test\"]:\n",
    "            raise ValueError(\"Type must be train, validation, or test\")\n",
    "        if name not in [\"X\", \"Y\"]:\n",
    "            raise ValueError(\"Name must be X or Y\")\n",
    "        if subtype not in [\"scaled\", None]:\n",
    "            raise ValueError(\"Subtype must be scaled or none\")\n",
    "\n",
    "        att_name = name + \"_\" + type if subtype is None else name + \"_\" + type + \"_\" + subtype\n",
    "\n",
    "        return getattr(self, att_name)\n",
    "    \n",
    "\n",
    "class Dataset:\n",
    "\n",
    "    def __init__(self,df,cat_columns,date_columns,cont_columns,disc_column,label_column=None):\n",
    "        self._df=df\n",
    "        self.cat_columns= cat_columns\n",
    "        self.date_columns= date_columns\n",
    "        self.cont_columns= cont_columns\n",
    "        self.disc_columns= disc_column\n",
    "        \n",
    "        if isinstance(label_column,str):\n",
    "            label_column=[label_column]\n",
    "            \n",
    "        if label_column is not None:\n",
    "            self.labeled=True\n",
    "            self.label_column=label_column\n",
    "        else:\n",
    "            self.labeled=False\n",
    "            self.label_column=[]\n",
    "            \n",
    "        \n",
    "    def check_columns(self,all_columns,*columns_groups):\n",
    "        columns=[]\n",
    "        for group in columns_groups:\n",
    "            if group:\n",
    "                columns+=group\n",
    "        if len(all_columns)==len(columns):\n",
    "\n",
    "            raise ValueError(f\"Columns do not match: {set(all_columns).difference(columns)}\")        \n",
    "        \n",
    "       \n",
    "    @property\n",
    "    def not_label_columns(self,date=False):\n",
    "        if not date:\n",
    "            return self.cat_columns+self.cont_columns+self.disc_columns\n",
    "        else:\n",
    "            return self.cat_columns+self.date_columns+self.cont_columns+self.disc_columns\n",
    "\n",
    "\n",
    "    @property\n",
    "    def df(self):\n",
    "        return self._df\n",
    "    \n",
    "    @df.setter\n",
    "    def df(self,df):\n",
    "        self._df=df\n",
    "\n",
    "    def set_sample(self,sample,type,name,subtype=None):\n",
    "        if type  not in [\"train\",\"validation\",\"test\"]:\n",
    "            raise ValueError(\"Type must be train, validation or test\")\n",
    "        if name not in [\"X\",\"Y\"]:\n",
    "            raise ValueError(\"Name must be X or Y\")\n",
    "        if subtype not in [\"scaled\",None]:\n",
    "            raise ValueError(\"Subtype must be scaled or none\")\n",
    "        \n",
    "        att_name=name+\"_\"+type if subtype is None else name+\"_\"+type+\"_\"+subtype\n",
    "\n",
    "        setattr(self,att_name,sample)\n",
    "    \n",
    "    def get_sample(self,type,name,subtype=None):\n",
    "        if type  not in [\"train\",\"validation\",\"test\"]:\n",
    "            raise ValueError(\"Type must be train, validation or test\")\n",
    "        if name not in [\"X\",\"Y\"]:\n",
    "            raise ValueError(\"Name must be X or Y\")\n",
    "        if subtype not in [\"scaled\",None]:\n",
    "            raise ValueError(\"Subtype must be scaled or none\")\n",
    "\n",
    "                \n",
    "        att_name=name+\"_\"+type if subtype is None else name+\"_\"+type+\"_\"+subtype\n",
    "\n",
    "        return getattr(self,att_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loader:\n",
    "    \"\"\"\n",
    "    A class that loads and processes a dataset.\n",
    "\n",
    "    Parameters:\n",
    "    path (str): The path to the CSV file containing the dataset.\n",
    "    cat_columns (list): A list of column names that are categorical variables.\n",
    "    date_columns (list): A list of column names that are date variables.\n",
    "    cont_columns (list): A list of column names that are continuous variables.\n",
    "    disc_columns (list): A list of column names that are discrete variables.\n",
    "    label_columns (list, optional): A list of column names that are labels. Defaults to None.\n",
    "\n",
    "    Attributes:\n",
    "    _dataset (Dataset): An instance of the Dataset class that represents the loaded dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path, cat_columns, date_columns, cont_columns, disc_columns, label_columns=None):\n",
    "        \"\"\"\n",
    "        Initializes a Loader object.\n",
    "\n",
    "        Loads the dataset from the specified CSV file and creates a Dataset object.\n",
    "\n",
    "        Parameters:\n",
    "        path (str): The path to the CSV file containing the dataset.\n",
    "        cat_columns (list): A list of column names that are categorical variables.\n",
    "        date_columns (list): A list of column names that are date variables.\n",
    "        cont_columns (list): A list of column names that are continuous variables.\n",
    "        disc_columns (list): A list of column names that are discrete variables.\n",
    "        label_columns (list, optional): A list of column names that are labels. Defaults to None.\n",
    "        \"\"\"\n",
    "        df = pd.read_csv(path, dtype=str)\n",
    "        self._dataset = Dataset(df, cat_columns, date_columns, cont_columns, disc_columns, label_columns)\n",
    "\n",
    "    def rename_columns(self, columns_map):\n",
    "        \"\"\"\n",
    "        Renames the columns of the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        columns_map (dict): A dictionary mapping old column names to new column names.\n",
    "        \"\"\"\n",
    "        self._dataset.df = self._dataset.df.rename(columns=columns_map)\n",
    "        \n",
    "    @property\n",
    "    def dataset(self):\n",
    "        \"\"\"\n",
    "        Returns the dataset.\n",
    "\n",
    "        Returns:\n",
    "        Dataset: An instance of the Dataset class representing the loaded dataset.\n",
    "        \"\"\"\n",
    "        return self._dataset\n",
    "    \n",
    "\n",
    "\n",
    "class Loader:\n",
    "\n",
    "    def __init__(self,path,cat_columns,date_columns,cont_columns,disc_columns,label_columns=None):\n",
    "        df=pd.read_csv(path,dtype=str)\n",
    "        self._dataset=Dataset(df,cat_columns,date_columns,cont_columns,disc_columns,label_columns)\n",
    "\n",
    "    def rename_columns(self,columns_map):\n",
    "        self._dataset.df=self._dataset.df.rename(columns=columns_map)\n",
    "        \n",
    "    @property\n",
    "    def dataset(self):\n",
    "        return self._dataset\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessamaneto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    \"\"\"\n",
    "    A class that provides preprocessing methods for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (Dataset): The dataset object containing the data to be preprocessed.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The dataset object containing the data to be preprocessed.\n",
    "        df (pandas.DataFrame): The DataFrame representation of the dataset.\n",
    "        cat_columns (list): The list of categorical column names in the dataset.\n",
    "        date_columns (list): The list of date column names in the dataset.\n",
    "        cont_columns (list): The list of continuous column names in the dataset.\n",
    "        disc_columns (list): The list of discrete column names in the dataset.\n",
    "\n",
    "    Methods:\n",
    "        _convert_cat(): Converts categorical columns to the 'category' data type.\n",
    "        _convert_date(): Converts date columns to the 'datetime' data type.\n",
    "        _convert_cont(): Converts continuous columns to the 'float' data type.\n",
    "        _convert_discrete(): Converts discrete columns to the 'float' and then 'int' data type.\n",
    "        fill_na(fill_value=\"\"): Fills missing values in the dataset with the specified fill value.\n",
    "        na_analysis(): Performs missing value analysis and returns the count of missing values for each column.\n",
    "        drop_na(): Drops rows with missing values from the dataset.\n",
    "        replacements(columns, replacements): Replaces substrings in the specified columns with the specified replacements.\n",
    "        create_dummies(columns): Creates dummy variables for the specified categorical columns.\n",
    "        create_date(day_column, month_column, year_column, date_column=None): Creates a new date column from the specified day, month, and year columns.\n",
    "        process_date(): Processes date columns and extracts additional features such as weekday, day, month, and year.\n",
    "        process_labels(): Converts the label column to categorical codes.\n",
    "        cat_to_codes(columns=None): Converts categorical columns to categorical codes.\n",
    "        set_types(): Sets the data types of the columns in the dataset.\n",
    "        update_dataset(): Updates the dataset object with the preprocessed data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset: Dataset):\n",
    "        self.dataset = dataset\n",
    "        self.df = dataset.df\n",
    "        self.cat_columns = dataset.cat_columns\n",
    "        self.date_columns = dataset.date_columns\n",
    "        self.cont_columns = dataset.cont_columns\n",
    "        self.disc_columns = dataset.disc_columns\n",
    "        self.dataset.df\n",
    "\n",
    "    def _convert_cat(self):\n",
    "        self.df.loc[:, self.cat_columns] = self.df[self.cat_columns].astype('category')\n",
    "\n",
    "    def _convert_date(self):\n",
    "        for col in self.date_columns:\n",
    "            self.df[col] = pd.to_datetime(self.df[col], errors='coerce')\n",
    "\n",
    "    def _convert_cont(self):\n",
    "        self.df[self.cont_columns] = self.df[self.cont_columns].astype(float)\n",
    "\n",
    "    def _convert_discrete(self):\n",
    "        self.df[self.disc_columns] = self.df[self.disc_columns].astype(float).astype(int)\n",
    "\n",
    "    def fill_na(self, fill_value=\"\"):\n",
    "        self.df = self.df.fillna(fill_value)\n",
    "\n",
    "    def na_analysis(self):\n",
    "        return self.df.isna().sum()\n",
    "\n",
    "    def drop_na(self):\n",
    "        self.df = self.df.dropna()\n",
    "\n",
    "    def replacements(self, columns, replacements):\n",
    "        if isinstance(replacements, str):\n",
    "            replacements = [replacements] * len(columns)\n",
    "        if isinstance(columns, str):\n",
    "            columns = [columns]\n",
    "        if len(columns) != len(replacements):\n",
    "            raise ValueError(\"Columns and replacements must have the same length\")\n",
    "\n",
    "        for column, replacement in zip(columns, replacements):\n",
    "            self.df[column] = self.df[column].str.replace(replacement, \"\")\n",
    "\n",
    "    def create_dummies(self, columns):\n",
    "        new_columns = []\n",
    "        for column in columns:\n",
    "            new_columns += list(map(lambda x: column + \"_\" + x, self.df[column].unique().tolist()))\n",
    "        self.df = pd.get_dummies(self.df, columns=columns)\n",
    "        for column in columns:\n",
    "            if column in self.cat_columns:\n",
    "                self.cat_columns.remove(column)\n",
    "        self.cat_columns += new_columns\n",
    "\n",
    "    def create_date(self, day_column, month_column, year_column, date_column=None):\n",
    "        if date_column is None:\n",
    "            date_column = \"data_\" + day_column.split(\"_\")[-1]\n",
    "        self.df[date_column] = pd.to_datetime(self.df[day_column].astype(float).astype(int).astype(str) \\\n",
    "                                              + \"-\" + self.df[month_column].astype(float).astype(int).astype(str) \\\n",
    "                                              + \"-\" + self.df[year_column].astype(float).astype(int).astype(str) \\\n",
    "                                              , errors='coerce')\n",
    "        self.date_columns.append(date_column)\n",
    "\n",
    "    def process_date(self):\n",
    "        for name, func in zip([\"weekday\", \"day\", \"month\", \"year\"],\n",
    "                              [lambda x: x.dt.weekday, lambda x: x.dt.day, lambda x: x.dt.month, lambda x: x.dt.year]):\n",
    "            columns = list(map(lambda x: x + \"_\" + name, self.date_columns))\n",
    "            self.df[columns] = self.df[self.date_columns].apply(func)\n",
    "            self.disc_columns += columns\n",
    "\n",
    "    def process_labels(self):\n",
    "        self.df[self.dataset.label_column] = self.df[self.dataset.label_column].apply(\n",
    "            lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "    def cat_to_codes(self, columns=None):\n",
    "        if columns is None:\n",
    "            columns = self.cat_columns\n",
    "        print(columns)\n",
    "        self.df[columns] = self.df[columns].apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "    def set_types(self):\n",
    "        # self._convert_cat()\n",
    "        self._convert_date()\n",
    "        self._convert_cont()\n",
    "        self._convert_discrete()\n",
    "\n",
    "    def update_dataset(self):\n",
    "        self.dataset.df = self.df\n",
    "        self.dataset.cat_columns = self.cat_columns\n",
    "        self.dataset.date_columns = self.date_columns\n",
    "        self.dataset.cont_columns = self.cont_columns\n",
    "        self.dataset.disc_columns = self.disc_columns\n",
    "\n",
    "        \n",
    "class   Preprocessor:\n",
    "    def __init__(self,dataset:Dataset):\n",
    "        self.dataset=dataset\n",
    "        self.df=dataset.df\n",
    "        self.cat_columns= self.dataset.cat_columns\n",
    "        self.date_columns= self.dataset.date_columns\n",
    "        self.cont_columns= self.dataset.cont_columns\n",
    "        self.disc_columns= self.dataset.disc_columns\n",
    "        self.dataset.df\n",
    "        \n",
    "    def _convert_cat(self):\n",
    "        self.df.loc[:,self.cat_columns]=self.df[self.cat_columns].astype('category')\n",
    "    \n",
    "    def _convert_date(self):\n",
    "        for col in self.date_columns:\n",
    "            self.df[col]=pd.to_datetime(self.df[col],errors='coerce')\n",
    "    \n",
    "    def _convert_cont(self):\n",
    "        self.df[self.cont_columns]=self.df[self.cont_columns].astype(float)\n",
    "  \n",
    "\n",
    "    def _covert_discrete(self):\n",
    "        self.df[self.disc_columns]=self.df[self.disc_columns].astype(float).astype(int)\n",
    "       \n",
    "\n",
    "    def fill_na(self,fill_value=\"\"):\n",
    "        self.df=self.df.fillna(fill_value)\n",
    "    \n",
    "    def na_analysis(self):\n",
    "        return self.df.isna().sum()\n",
    "    \n",
    "    def drop_na(self):\n",
    "        self.df=self.df.dropna()\n",
    "    \n",
    "    \n",
    "    def replacements(self,columns,replacements):\n",
    "        if isinstance(replacements,str):\n",
    "            replacements=[replacements]*len(columns)\n",
    "        if isinstance(columns,str):\n",
    "            columns=[columns]\n",
    "        if len(columns)!=len(replacements):\n",
    "            raise ValueError(\"Columns and replacements must have the same length\")\n",
    "\n",
    "        for column,replacement in zip(columns,replacements):\n",
    "            self.df[column]=self.df[column].str.replace(replacement,\"\")\n",
    "\n",
    "    def create_dummies(self,columns):\n",
    "        new_columns=[]\n",
    "        for column in columns: new_columns+=list(map(lambda x:column+\"_\"+x, self.df[column].unique().tolist()))\n",
    "        self.df=pd.get_dummies(self.df,columns=columns)\n",
    "        for column in columns:\n",
    "            if column in self.cat_columns:\n",
    "                self.cat_columns.remove(column)\n",
    "        self.cat_columns+=new_columns        \n",
    "    \n",
    "    def create_date(self,day_column,month_column,year_column,date_column=None):\n",
    "        if date_column is None:\n",
    "            date_column=\"data_\"+day_column.split(\"_\")[-1]\n",
    "        self.df[date_column]=pd.to_datetime(self.df[day_column].astype(float).astype(int).astype(str)\\\n",
    "                                                +\"-\"+self.df[month_column].astype(float).astype(int).astype(str)\\\n",
    "                                                +\"-\"+self.df[year_column].astype(float).astype(int).astype(str)\\\n",
    "                                        ,errors='coerce')\n",
    "        self.date_columns.append(date_column)\n",
    "    \n",
    "\n",
    "    def process_date(self):        \n",
    "        for name,func in zip([\"weekday\",\"day\",\"month\",\"year\"],[lambda x: x.dt.weekday,lambda x: x.dt.day,lambda x: x.dt.month,lambda x: x.dt.year]):\n",
    "            columns=list(map(lambda x: x+\"_\"+name,self.date_columns))\n",
    "            self.df[columns]=self.df[self.date_columns].apply(func)\n",
    "            self.disc_columns+=columns\n",
    "    \n",
    "    def process_labels(self):\n",
    "        self.df[self.dataset.label_column]=self.df[self.dataset.label_column].apply(lambda x: x.astype('category').cat.codes)\n",
    "\n",
    "    def cat_to_codes(self,columns=None):\n",
    "        if columns is None:\n",
    "            columns=self.cat_columns\n",
    "        print(columns)\n",
    "        self.df[columns]=self.df[columns].apply(lambda x: x.astype('category').cat.codes)\n",
    "        # for column in columns:\n",
    "            # self.df.iloc[:,column]=self.df[column].astype(\"category\").cat.codes\n",
    "        \n",
    "    def set_types(self):       \n",
    "        # self._convert_cat()\n",
    "        self._convert_date()\n",
    "        self._convert_cont()\n",
    "        self._covert_discrete()     \n",
    "\n",
    "    def update_dataset(self):\n",
    "        self.dataset.df=self.df\n",
    "        self.dataset.cat_columns=self.cat_columns\n",
    "        self.dataset.date_columns=self.date_columns\n",
    "        self.dataset.cont_columns=self.cont_columns\n",
    "        self.dataset.disc_columns=self.disc_columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessamneto de machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLPreprocessing():\n",
    "    \"\"\"\n",
    "    Class for performing preprocessing tasks on machine learning datasets.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The dataset object containing the data.\n",
    "        scaler (object): The scaler object used for feature scaling.\n",
    "\n",
    "    Methods:\n",
    "        undersample: Undersamples the dataset to balance the classes.\n",
    "        set_scaler: Sets the scaler object for feature scaling.\n",
    "        scale: Scales the features in the dataset.\n",
    "        split: Splits the dataset into train, validation, and test sets.\n",
    "        cross_validation: Performs cross-validation on the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def _split(X, Y, **kwargs):\n",
    "        \"\"\"\n",
    "        Helper function to split the dataset into train and test sets.\n",
    "\n",
    "        Args:\n",
    "            X (DataFrame): The input features.\n",
    "            Y (Series): The target variable.\n",
    "            **kwargs: Additional arguments to pass to the train_test_split function.\n",
    "\n",
    "        Returns:\n",
    "            X_train (DataFrame): The training set input features.\n",
    "            X_test (DataFrame): The test set input features.\n",
    "            Y_train (Series): The training set target variable.\n",
    "            Y_test (Series): The test set target variable.\n",
    "        \"\"\"\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, **kwargs)\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        \"\"\"\n",
    "        Initializes the MLPreprocessing object.\n",
    "\n",
    "        Args:\n",
    "            dataset (Dataset): The dataset object containing the data.\n",
    "        \"\"\"\n",
    "        self.dataset = dataset\n",
    "        self.dataset.__setattr__(\"scalable_columns\", self.dataset.cont_columns + self.dataset.disc_columns)\n",
    "        self.dataset.__setattr__(\"not_scalable_columns\", self.dataset.cat_columns)\n",
    "\n",
    "    def undersample(self):\n",
    "        \"\"\"\n",
    "        Undersamples the dataset to balance the classes.\n",
    "        \"\"\"\n",
    "        positive_sample_size = (self.dataset.df[self.dataset.label_column[0]] == 1).sum()\n",
    "        self.dataset.__setattr__(\"full_df\", self.dataset.df)\n",
    "        self.dataset.df = pd.concat([\n",
    "            self.dataset.df[self.dataset.df[self.dataset.label_column[0]] == 0].sample(positive_sample_size),\n",
    "            self.dataset.df[self.dataset.df[self.dataset.label_column[0]] == 1]\n",
    "        ], axis=0)\n",
    "        print(positive_sample_size)\n",
    "\n",
    "    def set_scaler(self, type=\"minmax\"):\n",
    "        \"\"\"\n",
    "        Sets the scaler object for feature scaling.\n",
    "\n",
    "        Args:\n",
    "            type (str): The type of scaler to use. Default is \"minmax\".\n",
    "        \"\"\"\n",
    "        scalers = {\"minmax\": MinMaxScaler, \"standard\": StandardScaler, \"robust\": RobustScaler,\n",
    "                   \"normalizer\": RobustScaler}\n",
    "        self.scaler = scalers[type]()\n",
    "\n",
    "    def scale(self, sample=[\"train\", \"validation\", \"test\"]):\n",
    "        \"\"\"\n",
    "        Scales the features in the dataset.\n",
    "\n",
    "        Args:\n",
    "            sample (list): The samples to scale. Default is [\"train\", \"validation\", \"test\"].\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the scaler was not set or the train data was not set.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"scaler\"):\n",
    "            raise ValueError(\"Scaler was not set\")\n",
    "        if not hasattr(self.dataset, \"X_train\"):\n",
    "            raise ValueError(\"Train data was not set\")\n",
    "\n",
    "        X_train_scaled = self.scaler.fit_transform(self.dataset.X_train[self.dataset.scalable_columns].astype(float))\n",
    "        X_train_scaled = pd.concat([\n",
    "            self.dataset.X_train[self.dataset.not_scalable_columns].reset_index(drop=True),\n",
    "            pd.DataFrame(X_train_scaled, columns=self.dataset.scalable_columns)\n",
    "        ], axis=1)\n",
    "        self.dataset.set_sample(X_train_scaled, \"train\", \"X\", subtype=\"scaled\")\n",
    "\n",
    "        if \"test\" in sample and hasattr(self.dataset, \"X_test\"):\n",
    "            X_test_scaled = self.scaler.transform(self.dataset.X_test[self.dataset.scalable_columns])\n",
    "            X_test_scaled = pd.concat([\n",
    "                self.dataset.X_test[self.dataset.not_scalable_columns].reset_index(drop=True),\n",
    "                pd.DataFrame(X_test_scaled, columns=self.dataset.scalable_columns)\n",
    "            ], axis=1)\n",
    "            self.dataset.set_sample(X_test_scaled, \"test\", \"X\", subtype=\"scaled\")\n",
    "\n",
    "        if \"validation\" in sample and hasattr(self.dataset, \"X_validation\"):\n",
    "            X_val_scaled = self.scaler.transform(self.dataset.X_validation[self.dataset.scalable_columns])\n",
    "            X_val_scaled = pd.concat([\n",
    "                self.dataset.X_validation[self.dataset.not_scalable_columns].reset_index(drop=True),\n",
    "                pd.DataFrame(X_val_scaled, columns=self.dataset.scalable_columns)\n",
    "            ], axis=1)\n",
    "            self.dataset.set_sample(X_val_scaled, \"validation\", \"X\", subtype=\"scaled\")\n",
    "\n",
    "    def split(self, validation=True, stratified=True, columns_stratify=None):\n",
    "        \"\"\"\n",
    "        Splits the dataset into train, validation, and test sets.\n",
    "\n",
    "        Args:\n",
    "            validation (bool): Whether to include a validation set. Default is True.\n",
    "            stratified (bool): Whether to perform stratified sampling. Default is True.\n",
    "            columns_stratify (list): The columns to use for stratified sampling. Default is None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the dataset is not labeled.\n",
    "            NotImplemented: If stratified sampling is requested but columns_stratify is not provided.\n",
    "        \"\"\"\n",
    "        train_size = 0.7 if validation else 0.80\n",
    "        validation_size = 0.15\n",
    "        test_size = 0.15 if validation else 0.20\n",
    "        if not self.dataset.labeled:\n",
    "            raise ValueError(\"Dataset is not labeled\")\n",
    "\n",
    "        if stratified and columns_stratify is not None:\n",
    "            columns_stratify += self.dataset.label_column\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = MLPreprocessing._split(\n",
    "            self.dataset.df[self.dataset.not_label_columns],\n",
    "            self.dataset.df[self.dataset.label_column],\n",
    "            random_state=42,\n",
    "            test_size=test_size + validation_size,\n",
    "            stratify=columns_stratify\n",
    "        )\n",
    "        if validation:\n",
    "            X_test, X_val, Y_test, Y_val = MLPreprocessing._split(\n",
    "                X_test, Y_test, random_state=42, test_size=validation_size / (test_size + validation_size),\n",
    "                stratify=columns_stratify\n",
    "            )\n",
    "        else:\n",
    "            X_val, Y_val = None, None\n",
    "\n",
    "        self.dataset.set_sample(X_train, \"train\", \"X\")\n",
    "        self.dataset.set_sample(Y_train, \"train\", \"Y\")\n",
    "        self.dataset.set_sample(X_val, \"validation\", \"X\")\n",
    "        self.dataset.set_sample(Y_val, \"validation\", \"Y\")\n",
    "        self.dataset.set_sample(X_test, \"test\", \"X\")\n",
    "        self.dataset.set_sample(Y_test, \"test\", \"Y\")\n",
    "\n",
    "    def cross_validation(self):\n",
    "        \"\"\"\n",
    "        Placeholder method for performing cross-validation on the dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class MLPreprocessing():\n",
    "\n",
    "\n",
    "    def _split(X,Y,**kwargs):\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test=train_test_split(X,Y,**kwargs)\n",
    "\n",
    "        return X_train, X_test, Y_train, Y_test\n",
    "   \n",
    "\n",
    "    def __init__(self,dataset:Dataset) -> None:\n",
    "        self.dataset=dataset\n",
    "        self.dataset.__setattr__(\"scalable_columns\",self.dataset.cont_columns+self.dataset.disc_columns)\n",
    "        self.dataset.__setattr__(\"not_scalable_columns\",self.dataset.cat_columns)\n",
    "\n",
    "    def undersample(self):\n",
    "        positive_sample_size=(self.dataset.df[self.dataset.label_column[0]]==1).sum()\n",
    "        self.dataset.__setattr__(\"full_df\",self.dataset.df)\n",
    "        self.dataset.df=pd.concat([self.dataset.df[self.dataset.df[self.dataset.label_column[0]]==0].sample(positive_sample_size),\n",
    "                                   self.dataset.df[self.dataset.df[self.dataset.label_column[0]]==1]],axis=0)\n",
    "        print(positive_sample_size)\n",
    "\n",
    "\n",
    "    def set_scaler(self,type=\"minmax\"):\n",
    "        scalers={\"minmax\":MinMaxScaler,\"standard\":StandardScaler,\"robust\":RobustScaler,\"normalizer\":RobustScaler}\n",
    "        self.scaler=scalers[type]()\n",
    "\n",
    "    def scale(self,sample=[\"train\",\"validation\",\"test\"]):\n",
    "\n",
    "        if not hasattr(self,\"scaler\"):\n",
    "            raise ValueError(\"Scaler was not set\")\n",
    "        if not hasattr(self.dataset,\"X_train\"):\n",
    "            raise ValueError(\"Train data was not set\")        \n",
    "\n",
    "        X_train_scaled=self.scaler.fit_transform(self.dataset.X_train[self.dataset.scalable_columns].astype(float))\n",
    "        X_train_scaled=pd.concat([self.dataset.X_train[self.dataset.not_scalable_columns].reset_index(drop=True),\n",
    "                                  pd.DataFrame(X_train_scaled,columns=self.dataset.scalable_columns)],axis=1)\n",
    "        self.dataset.set_sample(X_train_scaled,\"train\",\"X\",subtype=\"scaled\")\n",
    "\n",
    "        if \"test\" in sample and hasattr(self.dataset,\"X_test\"):\n",
    "            X_test_scaled=self.scaler.transform(self.dataset.X_test[self.dataset.scalable_columns])\n",
    "            X_test_scaled=pd.concat([self.dataset.X_test[self.dataset.not_scalable_columns].reset_index(drop=True),\n",
    "                                     pd.DataFrame(X_test_scaled,columns=self.dataset.scalable_columns)],axis=1)\n",
    "            self.dataset.set_sample(X_test_scaled,\"test\",\"X\",subtype=\"scaled\")\n",
    "\n",
    "        if \"validation\" in sample and hasattr(self.dataset,\"X_validation\"):\n",
    "            X_val_scaled=self.scaler.transform(self.dataset.X_validation[self.dataset.scalable_columns])\n",
    "            X_val_scaled=pd.concat([self.dataset.X_validation[self.dataset.not_scalable_columns].reset_index(drop=True),\n",
    "                                    pd.DataFrame(X_val_scaled,columns=self.dataset.scalable_columns)],axis=1)\n",
    "            self.dataset.set_sample(X_val_scaled,\"validation\",\"X\",subtype=\"scaled\")\n",
    "    \n",
    "\n",
    "    def split(self,validation=True,stratified=True,columns_stratify=None):\n",
    "\n",
    "         \n",
    "        train_size=0.7 if validation else 0.80\n",
    "        validation_size=0.15\n",
    "        test_size=0.15 if validation else 0.20\n",
    "        if not self.dataset.labeled:\n",
    "            raise NotImplemented()\n",
    "        \n",
    "        if stratified and columns_stratify is not None:            \n",
    "            columns_stratify+=self.dataset.label_column\n",
    "\n",
    "              \n",
    "        X_train, X_test, Y_train, Y_test=MLPreprocessing._split(self.dataset.df[self.dataset.not_label_columns],self.dataset.df[self.dataset.label_column],random_state=42,test_size=test_size+validation_size,stratify=columns_stratify)        \n",
    "        if validation:\n",
    "                X_test, X_val, Y_test, Y_val=MLPreprocessing._split(X_test,Y_test,random_state=42,test_size=validation_size/(test_size+validation_size),stratify=columns_stratify)\n",
    "        else:\n",
    "            X_val,Y_val=None,None\n",
    "        \n",
    "        self.dataset.set_sample(X_train,\"train\",\"X\")\n",
    "        self.dataset.set_sample(Y_train,\"train\",\"Y\")\n",
    "        self.dataset.set_sample(X_val,\"validation\",\"X\")\n",
    "        self.dataset.set_sample(Y_val,\"validation\",\"Y\")\n",
    "        self.dataset.set_sample(X_test,\"test\",\"X\")\n",
    "        self.dataset.set_sample(Y_test,\"test\",\"Y\")\n",
    "\n",
    "\n",
    "\n",
    "    def cross_validation():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseEstimator):\n",
    "    \"\"\"\n",
    "    A class representing a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be used.\n",
    "    - params: The parameters for the model.\n",
    "    - supervised: A boolean indicating whether the model is supervised or not.\n",
    "    - run_scaled: A boolean indicating whether to run the model on scaled data.\n",
    "    - run_on_categorical: A boolean indicating whether to run the model on categorical columns.\n",
    "    - run_on_continues: A boolean indicating whether to run the model on continuous columns.\n",
    "    - created: A boolean indicating whether the model has been created or not.\n",
    "\n",
    "    Methods:\n",
    "    - camel_case_split: Splits a camel case string into separate words.\n",
    "    - __init__: Initializes the Model object.\n",
    "    - get_X: Retrieves the X data for a given type and dataset.\n",
    "    - set_params: Sets the parameters for the model.\n",
    "    - grid_search_params: Sets the parameters for grid search.\n",
    "    - random_grid_search_params: Sets the parameters for random grid search.\n",
    "    - model: Returns the model object.\n",
    "    - set_metrics: Sets the metrics for the model.\n",
    "    - show_metrics: Returns the metrics for the model.\n",
    "    - fit: Fits the model to the data.\n",
    "    - __str__: Returns a string representation of the model.\n",
    "    - __call__: Makes predictions using the model.\n",
    "\n",
    "    Attributes:\n",
    "    - _model: The machine learning model.\n",
    "    - params: The parameters for the model.\n",
    "    - supervised: A boolean indicating whether the model is supervised or not.\n",
    "    - created: A boolean indicating whether the model has been created or not.\n",
    "    - run_scaled: A boolean indicating whether to run the model on scaled data.\n",
    "    - run_on_categorical: A boolean indicating whether to run the model on categorical columns.\n",
    "    - run_on_continues: A boolean indicating whether to run the model on continuous columns.\n",
    "    - grid_search_params: The parameters for grid search.\n",
    "    - metrics: The metrics for the model.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def camel_case_split(str):     \n",
    "        \"\"\"\n",
    "        Splits a camel case string into separate words.\n",
    "\n",
    "        Parameters:\n",
    "        - str: The camel case string to be split.\n",
    "\n",
    "        Returns:\n",
    "        - A list of words.\n",
    "        \"\"\"\n",
    "        start_idx = [i for i, e in enumerate(str)\n",
    "                    if e.isupper()] + [len(str)]\n",
    "    \n",
    "        start_idx = [0] + start_idx\n",
    "        return [str[x: y] for x, y in zip(start_idx, start_idx[1:])]         \n",
    "    \n",
    "    \n",
    "    def __init__(self,model,params,supervised:bool,run_scaled:bool=False,run_on_categorical=True,run_on_continues=True,created=False) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the Model object.\n",
    "\n",
    "        Parameters:\n",
    "        - model: The machine learning model to be used.\n",
    "        - params: The parameters for the model.\n",
    "        - supervised: A boolean indicating whether the model is supervised or not.\n",
    "        - run_scaled: A boolean indicating whether to run the model on scaled data.\n",
    "        - run_on_categorical: A boolean indicating whether to run the model on categorical columns.\n",
    "        - run_on_continues: A boolean indicating whether to run the model on continuous columns.\n",
    "        - created: A boolean indicating whether the model has been created or not.\n",
    "        \"\"\"\n",
    "        self._model=model\n",
    "        self.params=params\n",
    "        self.supervised=supervised\n",
    "        if not created:\n",
    "            self.set_params()\n",
    "        else:\n",
    "            self.created=created\n",
    "        self.run_scaled=run_scaled\n",
    "        self.run_on_categorical=run_on_categorical\n",
    "        self.run_on_continues=run_on_continues\n",
    "        if not (run_on_categorical or run_on_continues):\n",
    "            raise ValueError(\"Model must run on categorical and/or continuous columns\")\n",
    "        \n",
    "    def get_X(self,type:Union[\"train\",\"test\",\"validation\"],dataset:Dataset):       \n",
    "        \"\"\"\n",
    "        Retrieves the X data for a given type and dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - type: The type of data to retrieve (train, test, or validation).\n",
    "        - dataset: The dataset object.\n",
    "\n",
    "        Returns:\n",
    "        - The X data.\n",
    "        \"\"\"\n",
    "        subtype=\"scaled\" if self.run_scaled else None\n",
    "        columns=dataset.not_label_columns\n",
    "        if not self.run_on_categorical:\n",
    "            columns=list(set(columns).difference(dataset.cat_columns))\n",
    "        if not self.run_on_continues:\n",
    "            columns=list(set(columns).difference(dataset.cont_columns))\n",
    "            \n",
    "        return dataset.get_sample(type,\"X\",subtype=subtype)[columns]\n",
    "    \n",
    "    def set_params(self):\n",
    "        \"\"\"\n",
    "        Sets the parameters for the model.\n",
    "        \"\"\"\n",
    "        self.created=True\n",
    "        self._model=self._model(**self.params)\n",
    "    \n",
    "    def grid_search_params(self,**params):\n",
    "        \"\"\"\n",
    "        Sets the parameters for grid search.\n",
    "\n",
    "        Parameters:\n",
    "        - params: The parameters for grid search.\n",
    "        \"\"\"\n",
    "        self.grid_search_params=params\n",
    "\n",
    "    def random_grid_search_params(self,**params):\n",
    "        \"\"\"\n",
    "        Sets the parameters for random grid search.\n",
    "\n",
    "        Parameters:\n",
    "        - params: The parameters for random grid search.\n",
    "        \"\"\"\n",
    "        self.grid_search_params=params\n",
    "\n",
    "    @property\n",
    "    def model(self):\n",
    "        \"\"\"\n",
    "        Returns the model object.\n",
    "\n",
    "        Raises:\n",
    "        - ValueError: If the model has not been set.\n",
    "        \"\"\"\n",
    "        if not hasattr(self,\"created\"):\n",
    "            raise ValueError(\"Model was not set\")\n",
    "        \n",
    "        return self._model        \n",
    "\n",
    "    def set_metrics(self,metric,value):\n",
    "        \"\"\"\n",
    "        Sets the metrics for the model.\n",
    "\n",
    "        Parameters:\n",
    "        - metric: The metric name.\n",
    "        - value: The metric value.\n",
    "        \"\"\"\n",
    "        if not hasattr(self,\"metrics\"):\n",
    "            self.metrics={}\n",
    "        self.metrics[metric]=value\n",
    "\n",
    "    def show_metrics(self):\n",
    "        \"\"\"\n",
    "        Returns the metrics for the model.\n",
    "        \"\"\"\n",
    "        return self.metrics\n",
    "    \n",
    "    def fit(self,Y,X=None,type=None,dataset:Dataset=None,**kwargs):\n",
    "        \"\"\"\n",
    "        Fits the model to the data.\n",
    "\n",
    "        Parameters:\n",
    "        - Y: The target variable.\n",
    "        - X: The input data.\n",
    "        - type: The type of data (train, test, or validation).\n",
    "        - dataset: The dataset object.\n",
    "        - kwargs: Additional keyword arguments for the fit method.\n",
    "\n",
    "        Returns:\n",
    "        - The fitted model.\n",
    "        \"\"\"\n",
    "        if X is None and type is not None and dataset is not None:\n",
    "            X=self.get_X(type,dataset)\n",
    "            \n",
    "        return self._model.fit(X,Y,**kwargs)\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns a string representation of the model.\n",
    "        \"\"\"\n",
    "        name=(self.model.__str__()).split(\"(\")[0]\n",
    "        if self.run_scaled:\n",
    "            name+=\"_scaled\"\n",
    "        if self.run_on_categorical:\n",
    "            name+=\"_cat\"\n",
    "        if self.run_on_continues:\n",
    "            name+=\"_cont\"\n",
    "        return name\n",
    "            \n",
    "    def __call__(self,X=None,type=None,dataset:Dataset=None):\n",
    "        \"\"\"\n",
    "        Makes predictions using the model.\n",
    "\n",
    "        Parameters:\n",
    "        - X: The input data.\n",
    "        - type: The type of data (train, test, or validation).\n",
    "        - dataset: The dataset object.\n",
    "\n",
    "        Returns:\n",
    "        - The predictions.\n",
    "        \"\"\"\n",
    "        if type is not None and Dataset is not None:\n",
    "            X=self.get_X(type,dataset)\n",
    "        if X is None:\n",
    "            raise ValueError(\"X or type and Dataset must be passed\")\n",
    "        \n",
    "        return self._model.predict(X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinador de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\"\n",
    "    The Trainer class is responsible for training machine learning models.\n",
    "\n",
    "    Args:\n",
    "        models (List[BaseEstimator]): A list of machine learning models to be trained.\n",
    "        dataset (Dataset): The dataset used for training and evaluation.\n",
    "\n",
    "    Attributes:\n",
    "        models (List[BaseEstimator]): A list of machine learning models.\n",
    "        dataset (Dataset): The dataset used for training and evaluation.\n",
    "\n",
    "    Methods:\n",
    "        _train(model: Model, **kwargs): Trains a single machine learning model.\n",
    "        train(**kwargs): Trains all models or a specific model.\n",
    "        run_grid_search(random_state=123, n_iter=10): Runs grid search for models with grid search parameters.\n",
    "        run_evaluation(): Evaluates all models.\n",
    "        add(model: Model, train=True): Adds a new model to the list of models.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models: List[BaseEstimator], dataset: Dataset) -> None:\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "            \n",
    "        self.models = models\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def _train(self, model: Model, **kwargs):\n",
    "        \"\"\"\n",
    "        Trains a single machine learning model.\n",
    "\n",
    "        Args:\n",
    "            model (Model): The machine learning model to be trained.\n",
    "            **kwargs: Additional keyword arguments to be passed to the model's fit method.\n",
    "\n",
    "        \"\"\"\n",
    "        t1 = time.time()\n",
    "        if model.supervised:\n",
    "            model.fit(type=\"train\", dataset=self.dataset, Y=self.dataset.Y_train, **kwargs)\n",
    "        else:\n",
    "            model.fit(tpe=\"train\", dataset=self.dataset, **kwargs)\n",
    "             \n",
    "        t2 = time.time()\n",
    "        model.__setattr__(\"training_time\", t2 - t1)\n",
    "\n",
    "    def train(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Trains all models or a specific model.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: Additional keyword arguments to be passed to the _train method.\n",
    "                If \"model\" is provided, only that specific model will be trained.\n",
    "\n",
    "        \"\"\"\n",
    "        if \"model\" not in kwargs:            \n",
    "            for model in self.models:                    \n",
    "                self._train(model, **kwargs)\n",
    "        else:\n",
    "            model = kwargs.pop(\"model\")\n",
    "            self._train(model, **kwargs)\n",
    "\n",
    "\n",
    "    def run_evaluation(self):\n",
    "        \"\"\"\n",
    "        Evaluates all models.\n",
    "\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            model.evaluate()\n",
    "\n",
    "    def add(self, model: Model, train=True):\n",
    "        \"\"\"\n",
    "        Adds a new model to the list of models.\n",
    "\n",
    "        Args:\n",
    "            model (Model): The machine learning model to be added.\n",
    "            train (bool): Whether to train the model after adding it. Default is True.\n",
    "\n",
    "        \"\"\"\n",
    "        self.models.append(model)\n",
    "        if train:\n",
    "            self.train(model=model)\n",
    "\n",
    "            \n",
    "class Trainer():\n",
    "    def __init__(self,models:List[BaseEstimator],dataset:Dataset) -> None:\n",
    "        if not isinstance(models,list):\n",
    "            models=[models]\n",
    "            \n",
    "        self.models=models\n",
    "        self.dataset=dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _train(self,model:Model,**kwargs):\n",
    "            t1=time.time()\n",
    "            if model.supervised:\n",
    "                    model.fit(type=\"train\",dataset=self.dataset,Y=self.dataset.Y_train,**kwargs)\n",
    "            \n",
    "            else:\n",
    "                    model.fit(tpe=\"train\",dataset=self.dataset,**kwargs)\n",
    "             \n",
    "            t2=time.time()\n",
    "            model.__setattr__(\"training_time\",t2-t1)\n",
    "        \n",
    "\n",
    "    def train(self,**kwargs):\n",
    "        if \"model\" not in kwargs:            \n",
    "            for model in self.models:                    \n",
    "                self._train(model,**kwargs)\n",
    "        else:\n",
    "            model=kwargs.pop(\"model\")\n",
    "            self._train(model,**kwargs)\n",
    "        \n",
    "\n",
    "\n",
    "    def run_grid_search(self,random_state=123,n_iter=10):\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "        for model in self.models:\n",
    "            if not hasattr(model,\"grid_search_params\"):\n",
    "                continue\n",
    "            else:\n",
    "                parameters=ParameterSampler(model.grid_search_params,n_iter=n_iter,random_state=random_state)\n",
    "                for param in parameters:\n",
    "                    model.copy().set_params(**param)\n",
    "                    self._train(model)\n",
    "            \n",
    "                         \n",
    "\n",
    "    def run_evaluation(self):\n",
    "        for model in self.models:\n",
    "            model.evaluate()\n",
    "\n",
    "    def add(self,model:Model,train=True):\n",
    "        self.models.append(model)\n",
    "        if train:\n",
    "            self.train(model=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliação de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Evaluation():\n",
    "    \"\"\"\n",
    "    Class to perform evaluation of models on a dataset.\n",
    "\n",
    "    Args:\n",
    "        models (List[Model]): A list of models to evaluate.\n",
    "        dataset (Dataset): The dataset to evaluate the models on.\n",
    "\n",
    "    Attributes:\n",
    "        dataset (Dataset): The dataset used for evaluation.\n",
    "        models (List[Model]): The models to evaluate.\n",
    "\n",
    "    Methods:\n",
    "        run(sample=\"validation\"): Runs the evaluation on the specified sample.\n",
    "        metrics(): Returns the evaluation metrics for each model.\n",
    "        plot_metrics(orient=\"model\"): Plots the evaluation metrics for each model.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models: List[Model], dataset: Dataset) -> None:\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "            \n",
    "        self.dataset = dataset\n",
    "        self.models = models\n",
    "\n",
    "\n",
    "    def run(self, sample=\"validation\"):\n",
    "        \"\"\"\n",
    "        Runs the evaluation on the specified sample.\n",
    "\n",
    "        Args:\n",
    "            sample (str, optional): The sample to evaluate the models on. Defaults to \"validation\".\n",
    "\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            accuracy = accuracy_score(self.dataset.get_sample(sample, \"Y\"), model(type=sample, dataset=self.dataset))  \n",
    "            precision = precision_score(self.dataset.get_sample(sample, \"Y\"), model(type=sample, dataset=self.dataset))\n",
    "            recall = recall_score(self.dataset.get_sample(sample, \"Y\"), model(type=sample, dataset=self.dataset))\n",
    "            f1 = f1_score(self.dataset.get_sample(sample, \"Y\"), model(type=sample, dataset=self.dataset))\n",
    "            roc_auc = roc_auc_score(self.dataset.get_sample(sample, \"Y\"), model(type=sample, dataset=self.dataset))\n",
    "\n",
    "            model.set_metrics(\"accuracy\", accuracy)\n",
    "            model.set_metrics(\"precision\", precision)\n",
    "            model.set_metrics(\"recall\", recall)\n",
    "            model.set_metrics(\"f1\", f1)\n",
    "            model.set_metrics(\"roc_auc\", roc_auc)\n",
    "\n",
    "\n",
    "    def metrics(self):\n",
    "        \"\"\"\n",
    "        Returns the evaluation metrics for each model.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the evaluation metrics for each model.\n",
    "\n",
    "        \"\"\"\n",
    "        metrics = {}\n",
    "        for model in self.models:\n",
    "            metrics[str(model)] = model.show_metrics()\n",
    "        return metrics\n",
    "    \n",
    "    def plot_metrics(self, orient=\"model\"):\n",
    "        \"\"\"\n",
    "        Plots the evaluation metrics for each model.\n",
    "\n",
    "        Args:\n",
    "            orient (str, optional): The orientation of the plot. Defaults to \"model\".\n",
    "\n",
    "        Returns:\n",
    "            matplotlib.figure.Figure: The plotted figure.\n",
    "\n",
    "        \"\"\"\n",
    "        if orient == \"model\":\n",
    "            fig, axs = plt.subplot_mosaic([[\"accuracy\", \"precision\", \"recall\"], [\"f1\", \"roc_auc\", \"vazio\"]], sharey=True, figsize=(10, 4))\n",
    "            for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "                values = []\n",
    "                for model in self.models:\n",
    "                    values.append(model.__getattribute__(\"metrics\")[metric])\n",
    "                axs[metric].set_title(metric)\n",
    "                sns.barplot(x=values, y=[str(model) for model in self.models], ax=axs[metric])\n",
    "                axs[metric].set_xlim(0.5, 1)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "        else:\n",
    "            fig, axs = plt.subplots(len(self.models)//5+1, 5, sharey=True)\n",
    "            if len(self.models)//5+1 == 1:\n",
    "                axs = axs.reshape(1, -1)\n",
    "\n",
    "            for index, model in enumerate(self.models):\n",
    "                values = []\n",
    "                for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"]:\n",
    "                    values.append(model.__getattribute__(\"metrics\")[metric])\n",
    "                axs[index//5, index%5].set_title(model.model, fontsize=8)\n",
    "                sns.barplot(x=values, y=[\"accuracy\", \"precision\", \"recall\", \"f1\", \"roc_auc\"], ax=axs[index//5, index%5])\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "\n",
    "\n",
    "\n",
    "class Evaluation():\n",
    "    \n",
    "    def __init__(self,models:List[Model],dataset:Dataset) -> None:\n",
    "        if not isinstance(models,list):\n",
    "            models=[models]\n",
    "            \n",
    "        self.dataset=dataset\n",
    "        self.models=models\n",
    "\n",
    "\n",
    "    def run(self,sample=\"validation\"):\n",
    "        for model in self.models:\n",
    "           \n",
    "            accuracy=accuracy_score(self.dataset.get_sample(sample,\"Y\"),model(type=sample,dataset=self.dataset))  \n",
    "            precision=precision_score(self.dataset.get_sample(sample,\"Y\"),model(type=sample,dataset=self.dataset))\n",
    "            recall=recall_score(self.dataset.get_sample(sample,\"Y\"),model(type=sample,dataset=self.dataset))\n",
    "            f1=f1_score(self.dataset.get_sample(sample,\"Y\"),model(type=sample,dataset=self.dataset))\n",
    "            roc_auc=roc_auc_score(self.dataset.get_sample(sample,\"Y\"),model(type=sample,dataset=self.dataset))\n",
    "\n",
    "            model.set_metrics(\"accuracy\",accuracy)\n",
    "            model.set_metrics(\"precision\",precision)\n",
    "            model.set_metrics(\"recall\",recall)\n",
    "            model.set_metrics(\"f1\",f1)\n",
    "            model.set_metrics(\"roc_auc\",roc_auc)\n",
    "\n",
    "\n",
    "    def metrics(self):\n",
    "        metrics={}\n",
    "        for model in self.models:\n",
    "            metrics[str(model)]=model.show_metrics()\n",
    "        return metrics\n",
    "    \n",
    "    def plot_metrics(self,orient=\"model\"):\n",
    "        if orient==\"model\":\n",
    "            fig,axs=plt.subplot_mosaic([[\"accuracy\",\"precision\",\"recall\"],[\"f1\",\"roc_auc\",\"vazio\"]],sharey=True,figsize=(10,4))\n",
    "            for metric in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]:\n",
    "                values=[]\n",
    "                for model in self.models:\n",
    "                    values.append(model.__getattribute__(\"metrics\")[metric])\n",
    "                axs[metric].set_title(metric)\n",
    "                sns.barplot(x=values,y=[str(model) for model in self.models],ax=axs[metric])\n",
    "                axs[metric].set_xlim(0.5,1)\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "        else:\n",
    "            fig,axs=plt.subplots(len(self.models)//5+1,5,sharey=True)\n",
    "            if len(self.models)//5+1 ==1:\n",
    "                axs=axs.reshape(1,-1)\n",
    "\n",
    "            for index,model in enumerate(self.models):\n",
    "                values=[]\n",
    "                for metric in [\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"]:\n",
    "                    values.append(model.__getattribute__(\"metrics\")[metric])\n",
    "                axs[index//5,index%5].set_title(model.model,fontsize=8)\n",
    "                sns.barplot(x=values,y=[\"accuracy\",\"precision\",\"recall\",\"f1\",\"roc_auc\"],ax=axs[index//5,index%5])\n",
    "            plt.tight_layout()\n",
    "            return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção dos modelos treinados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Selector():\n",
    "    \"\"\"\n",
    "    A class that selects the best model based on a specified metric.\n",
    "    \n",
    "    Args:\n",
    "        models (List[Model]): A list of models to select from.\n",
    "        aimed_metric (str): The metric to optimize for. Defaults to \"accuracy\".\n",
    "    \n",
    "    Attributes:\n",
    "        models (List[Model]): A list of models to select from.\n",
    "        aimed_metric (str): The metric to optimize for.\n",
    "    \n",
    "    Methods:\n",
    "        check_metric(): Checks if each model has the specified metric.\n",
    "        select(): Selects the model with the highest value for the specified metric.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, models: List[Model], aimed_metric: str = \"accuracy\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes a Selector object.\n",
    "        \n",
    "        Args:\n",
    "            models (List[Model]): A list of models to select from.\n",
    "            aimed_metric (str): The metric to optimize for. Defaults to \"accuracy\".\n",
    "        \"\"\"\n",
    "        if not isinstance(models, list):\n",
    "            models = [models]\n",
    "        \n",
    "        self.models = models\n",
    "        self.aimed_metric = aimed_metric\n",
    "    \n",
    "    def check_metric(self):\n",
    "        \"\"\"\n",
    "        Checks if each model has the specified metric.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If a model does not have the specified metric.\n",
    "        \"\"\"\n",
    "        for model in self.models:\n",
    "            if not hasattr(model, \"metrics\"):\n",
    "                raise ValueError(f\"{str(model)} has no metrics\")\n",
    "            elif model.metrics.get(self.aimed_metric) is None:\n",
    "                raise ValueError(f\"{str(model)} has no metrics\")\n",
    "            \n",
    "    def select(self):\n",
    "        \"\"\"\n",
    "        Selects the model with the highest value for the specified metric.\n",
    "        \n",
    "        Returns:\n",
    "            Model: The selected model.\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for model in self.models:\n",
    "            scores.append(model.metrics.get(self.aimed_metric))\n",
    "        \n",
    "        argmax = scores.index(max(scores))\n",
    "\n",
    "        return self.models[argmax]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de Hiperparâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTunning:\n",
    "    \"\"\"\n",
    "    The ModelTunning class is used for hyperparameter tuning of machine learning models.\n",
    "    It provides methods for performing random search and grid search to find the best set of hyperparameters.\n",
    "\n",
    "    Attributes:\n",
    "        model (Model): The machine learning model to be tuned.\n",
    "        dataset (Dataset): The dataset used for training the model.\n",
    "        cv (int): The number of cross-validation folds.\n",
    "        random_state (int): The random seed for reproducibility.\n",
    "        scoring_fn (str): The scoring function used for evaluating the models.\n",
    "\n",
    "    Methods:\n",
    "        ideal_cutoff(size, cutoff, max_iter): Recursive function to calculate the ideal cutoff value.\n",
    "        RandomSearch(n_iter): Performs random search to find the best set of hyperparameters.\n",
    "        GridSearch(max_iter, amplitude, cutoff, params): Performs grid search to find the best set of hyperparameters.\n",
    "        best_model: Returns the best model found during the search.\n",
    "\n",
    "    Usage:\n",
    "        # Create an instance of ModelTunning\n",
    "        tuner = ModelTunning(model, dataset, cv=10, random_state=123, scoring_fn=\"accuracy\")\n",
    "\n",
    "        # Perform random search\n",
    "        tuner.RandomSearch(n_iter=10)\n",
    "\n",
    "        # Perform grid search\n",
    "        tuner.GridSearch(max_iter=30, amplitude=0.5, cutoff=5, params=None)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = tuner.best_model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model: Model, dataset: Dataset, cv=10, random_state=123, scoring_fn=\"accuracy\") -> None:\n",
    "        \"\"\"\n",
    "        Initializes a new instance of the ModelTunning class.\n",
    "\n",
    "        Args:\n",
    "            model (Model): The machine learning model to be tuned.\n",
    "            dataset (Dataset): The dataset used for training the model.\n",
    "            cv (int, optional): The number of cross-validation folds. Defaults to 10.\n",
    "            random_state (int, optional): The random seed for reproducibility. Defaults to 123.\n",
    "            scoring_fn (str, optional): The scoring function used for evaluating the models. Defaults to \"accuracy\".\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.dataset = dataset\n",
    "        self.random_state = random_state\n",
    "        self.cv = cv\n",
    "        self.scoring_fn = scoring_fn\n",
    "\n",
    "    def ideal_cutoff(size, cutoff, max_iter):\n",
    "        \"\"\"\n",
    "        Recursive function to calculate the ideal cutoff value.\n",
    "\n",
    "        Args:\n",
    "            size (int): The number of hyperparameters.\n",
    "            cutoff (int): The current cutoff value.\n",
    "            max_iter (int): The maximum number of iterations.\n",
    "\n",
    "        Returns:\n",
    "            int: The ideal cutoff value.\n",
    "        \"\"\"\n",
    "        if size * cutoff > max_iter:\n",
    "            cutoff -= 1\n",
    "            return ModelTunning.ideal_cutoff(size, cutoff, max_iter)\n",
    "        else:\n",
    "            return cutoff\n",
    "\n",
    "    def RandomSearch(self, n_iter=10):\n",
    "        \"\"\"\n",
    "        Performs random search to find the best set of hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            n_iter (int, optional): The number of iterations. Defaults to 10.\n",
    "\n",
    "        Returns:\n",
    "            dict: The results of the random search.\n",
    "        \"\"\"\n",
    "        search = RandomizedSearchCV(\n",
    "            self.model.model,\n",
    "            self.model.grid_search_params,\n",
    "            n_iter=n_iter,\n",
    "            random_state=self.random_state,\n",
    "            scoring=self.scoring_fn,\n",
    "            cv=self.cv\n",
    "        )\n",
    "\n",
    "        search.fit(self.model.get_X(type=\"train\", dataset=self.dataset), self.dataset.Y_train)\n",
    "        self.random_search = search\n",
    "        self.random_search_best_model = copy.deepcopy(self.model)\n",
    "        self.random_search_best_model._model = clone(self.random_search.best_estimator_)\n",
    "        return search.cv_results_\n",
    "\n",
    "    def GridSearch(self, max_iter=30, amplitude=0.5, cutoff=5, params=None):\n",
    "        \"\"\"\n",
    "        Performs grid search to find the best set of hyperparameters.\n",
    "\n",
    "        Args:\n",
    "            max_iter (int, optional): The maximum number of iterations. Defaults to 30.\n",
    "            amplitude (float, optional): The amplitude for generating parameter values. Defaults to 0.5.\n",
    "            cutoff (int, optional): The cutoff value for generating parameter values. Defaults to 5.\n",
    "            params (dict, optional): The hyperparameters to be tuned. If None, the best parameters from random search will be used. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict: The results of the grid search.\n",
    "        \"\"\"\n",
    "        if params is None:\n",
    "            if not hasattr(self, \"random_search\"):\n",
    "                raise ValueError(\"Params must be passed or RandomSearch must be run\")\n",
    "\n",
    "            params = self.random_search.best_params_\n",
    "            if max_iter < 3 * len(params):\n",
    "                print(\"Max_iter is less than the number of parameters. It is being set to 3 times the number of parameters\")\n",
    "                max_iter = 3 * len(params)\n",
    "\n",
    "            cutoff = self.ideal_cutoff(len(params), cutoff, max_iter)\n",
    "\n",
    "            for param, value in params.items():\n",
    "                if isinstance(value, int):\n",
    "                    params[param] = np.arange(value - min(cutoff, int(value * amplitude)), value + min(cutoff, int(value * amplitude)))\n",
    "                elif isinstance(value, float):\n",
    "                    params[param] = np.arange(value - min(cutoff, value * amplitude), value + min(cutoff, value * amplitude))\n",
    "\n",
    "        fine_search = GridSearchCV(\n",
    "            self.model.model,\n",
    "            params,\n",
    "            scoring=self.scoring_fn,\n",
    "            cv=self.cv\n",
    "        )\n",
    "\n",
    "        fine_search.fit(self.model.get_X(type=\"train\", dataset=self.dataset), self.dataset.Y_train)\n",
    "        self.fine_search = fine_search\n",
    "        self.grid_search_best_model = copy.deepcopy(self.model)\n",
    "        self.grid_search_best_model._model = clone(self.random_search.best_estimator_)\n",
    "        return fine_search.cv_results_\n",
    "\n",
    "    @property\n",
    "    def best_model(self):\n",
    "        \"\"\"\n",
    "        Returns the best model found during the search.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If no search was run.\n",
    "\n",
    "        Returns:\n",
    "            Model: The best model.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"grid_search_best_model\"):\n",
    "            return self.grid_search_best_model\n",
    "        elif hasattr(self, \"random_search_best_model\"):\n",
    "            return self.random_search_best_model\n",
    "        else:\n",
    "            raise ValueError(\"No search was run\")\n",
    "\n",
    "\n",
    "class ModelTunning:\n",
    "    \n",
    "    @staticmethod\n",
    "    def ideal_cutoff(size,cutoff,max_iter):\n",
    "        \n",
    "        if size*cutoff>max_iter:\n",
    "            cutoff-=1\n",
    "            return ModelTunning.ideal_cutoff(size,cutoff,max_iter)\n",
    "            #return cutoff\n",
    "        else:\n",
    "            return cutoff\n",
    "    def __init__(self,model:Model,dataset:Dataset,cv=10,random_state=123,scoring_fn=\"accuracy\") -> None:\n",
    "        self.model=model\n",
    "        self.dataset=dataset\n",
    "        self.random_state=random_state\n",
    "        self.cv=cv\n",
    "        self.scoring_fn=scoring_fn\n",
    "\n",
    "    def RandomSearch(self,n_iter=10):\n",
    "        search=RandomizedSearchCV(self.model.model,self.model.grid_search_params,n_iter=n_iter,random_state=self.random_state,\n",
    "                                  scoring=self.scoring_fn,cv=self.cv)\n",
    "        \n",
    "        search.fit(self.model.get_X(type=\"train\",dataset=self.dataset),self.dataset.Y_train)\n",
    "        self.random_search=search\n",
    "        self.random_search_best_model=copy.deepcopy(self.model)\n",
    "        self.random_search_best_model._model=clone(self.random_search.best_estimator_)\n",
    "        return search.cv_results_\n",
    "    \n",
    "\n",
    "    \n",
    "    def GridSearch(self,max_iter=30,amplitude=0.5,cutoff=5,params=None):\n",
    "        if params is None:            \n",
    "            if not hasattr(self,\"random_search\"):\n",
    "                raise ValueError(\"Params must be passed or RandomSearch must be run\")\n",
    "            \n",
    "            params=self.random_search.best_params_\n",
    "            if max_iter<3*len(params):\n",
    "                print(\"Max_iter is less than the number of parameters. It is being set to 3 times the number of parameters\")\n",
    "                max_iter=3*len(params)\n",
    "\n",
    "            \n",
    "            cutoff=self.ideal_cutoff(len(params),cutoff,max_iter)\n",
    "        \n",
    "            for param,value in params.items():                \n",
    "                if isinstance(value,int):\n",
    "                    params[param]=np.arange(value-min(cutoff,int(value*amplitude)),value+min(cutoff,int(value*amplitude)))\n",
    "                elif isinstance(value,float):\n",
    "                    params[param]=np.arange(value-min(cutoff,value*amplitude),value+min(cutoff,value*amplitude))\n",
    "\n",
    "                    \n",
    "        fine_search=GridSearchCV(self.model.model,params,scoring=self.scoring_fn,cv=self.cv)       \n",
    "        fine_search.fit(self.model.get_X(type=\"train\",dataset=self.dataset),self.dataset.Y_train)\n",
    "        self.fine_search=fine_search\n",
    "        self.grid_search_best_model=copy.deepcopy(self.model)\n",
    "        self.grid_search_best_model._model=clone(self.random_search.best_estimator_)\n",
    "        return fine_search.cv_results_\n",
    "    \n",
    "    @property\n",
    "    def best_model(self):\n",
    "        if hasattr(self,\"grid_search_best_model\"):\n",
    "            return self.grid_search_best_model\n",
    "        elif hasattr(self,\"random_search_best_model\"):\n",
    "            return self.random_search_best_model\n",
    "        else:\n",
    "            raise ValueError(\"No search was run\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Orchestrador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class orchestrator():\n",
    "    \"\"\"\n",
    "    The orchestrator class is responsible for coordinating the different steps of a machine learning pipeline.\n",
    "    It takes in various components such as models, data loader, preprocessor, trainer, evaluator, model tunning, and model selector.\n",
    "    The main purpose of this class is to provide a high-level interface to run the entire pipeline and obtain the best model.\n",
    "\n",
    "    Args:\n",
    "        models (list): A list of machine learning models to be trained and evaluated.\n",
    "        loader (object): An object that loads the dataset.\n",
    "        preprocessor_class (class): The class for preprocessing the dataset.\n",
    "        ml_reprocessing_class (class): The class for preprocessing the dataset for machine learning.\n",
    "        trainer_class (class): The class for training the models.\n",
    "        evaluation_class (class): The class for evaluating the models.\n",
    "        model_tunning_class (class): The class for tuning the hyperparameters of the models.\n",
    "        model_selector_class (class): The class for selecting the best model based on a decision metric.\n",
    "\n",
    "    Attributes:\n",
    "        models (list): A list of machine learning models.\n",
    "        loader (object): An object that loads the dataset.\n",
    "        preprocessing_class (class): The class for preprocessing the dataset.\n",
    "        ml_preprocessing_class (class): The class for preprocessing the dataset for machine learning.\n",
    "        trainer_class (class): The class for training the models.\n",
    "        evaluation_class (class): The class for evaluating the models.\n",
    "        tunning_class (class): The class for tuning the hyperparameters of the models.\n",
    "        model_selector_class (class): The class for selecting the best model based on a decision metric.\n",
    "        preprocessor (object): An object that performs preprocessing on the dataset.\n",
    "        MLpreprocessor (object): An object that performs preprocessing for machine learning on the dataset.\n",
    "        trainer (object): An object that trains the models.\n",
    "        evaluator (object): An object that evaluates the models.\n",
    "        best_model (object): The best model selected based on a decision metric.\n",
    "        tunning (object): An object that tunes the hyperparameters of the best model.\n",
    "        bestmodel_MLpreprocessor (object): An object that performs preprocessing for machine learning on the dataset for the best model.\n",
    "        bestmodel_trainer (object): An object that trains the best model.\n",
    "        bestmodel_evaluator (object): An object that evaluates the best model.\n",
    "\n",
    "    Methods:\n",
    "        run_preprocessing: Performs preprocessing on the dataset.\n",
    "        run_ml_preprocessing: Performs preprocessing for machine learning on the dataset.\n",
    "        run_training: Trains the models.\n",
    "        run_model_evaluation: Evaluates the models.\n",
    "        run_model_selection: Selects the best model based on a decision metric.\n",
    "        run_tuning: Tunes the hyperparameters of a model.\n",
    "        run_pipeline: Runs the entire machine learning pipeline.\n",
    "        run_retrain_best_model: Retrains the best model on the entire dataset.\n",
    "        pickle_model: Saves the best model to a file.\n",
    "        get_metrics: Returns the metrics of the best model.\n",
    "        plot_metrics: Plots the evaluation metrics of the best model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, models, loader, preprocessor_class: Preprocessor, ml_reprocessing_class: MLPreprocessing,\n",
    "                 trainer_class: Trainer, evaluation_class: Evaluation, model_tunning_class: ModelTunning,\n",
    "                 model_selector_class: Selector) -> None:\n",
    "        \"\"\"\n",
    "        Initializes the orchestrator class with the provided components.\n",
    "\n",
    "        Args:\n",
    "            models (list): A list of machine learning models to be trained and evaluated.\n",
    "            loader (object): An object that loads the dataset.\n",
    "            preprocessor_class (class): The class for preprocessing the dataset.\n",
    "            ml_reprocessing_class (class): The class for preprocessing the dataset for machine learning.\n",
    "            trainer_class (class): The class for training the models.\n",
    "            evaluation_class (class): The class for evaluating the models.\n",
    "            model_tunning_class (class): The class for tuning the hyperparameters of the models.\n",
    "            model_selector_class (class): The class for selecting the best model based on a decision metric.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.loader = loader\n",
    "        self.preprocessing_class = preprocessor_class\n",
    "        self.ml_preprocessing_class = ml_reprocessing_class\n",
    "        self.trainer_class = trainer_class\n",
    "        self.evaluation_class = evaluation_class\n",
    "        self.tunning_class = model_tunning_class\n",
    "        self.model_selector_class = model_selector_class\n",
    "\n",
    "    def run_preprocessing(self, replacement_columns=[\"tamanho_motor\", \"milhas_carro\"], replacements=[\"L\", \"mile\"],\n",
    "                          adv_day_column=\"dia_aviso\", adv_month_column=\"mes_aviso\", adv_year_column=\"ano_aviso\",\n",
    "                          dumies_columns=[\"cor\", \"tipo_cambio\"]):\n",
    "        \"\"\"\n",
    "        Performs preprocessing on the dataset.\n",
    "\n",
    "        Args:\n",
    "            replacement_columns (list): A list of column names to be replaced.\n",
    "            replacements (list): A list of replacement values corresponding to the replacement_columns.\n",
    "            adv_day_column (str): The column name for the day of the advertisement.\n",
    "            adv_month_column (str): The column name for the month of the advertisement.\n",
    "            adv_year_column (str): The column name for the year of the advertisement.\n",
    "            dumies_columns (list): A list of column names to be converted to dummy variables.\n",
    "\n",
    "        Returns:\n",
    "            object: An object that performs preprocessing on the dataset.\n",
    "        \"\"\"\n",
    "        preprocessor = self.preprocessing_class(self.loader.dataset)\n",
    "        preprocessor.replacements(columns=replacement_columns, replacements=replacements)\n",
    "        preprocessor.set_types()\n",
    "\n",
    "        if (adv_day_column in self.loader.dataset.df.columns and \"mes_aviso\" in self.loader.dataset.df.columns and\n",
    "                \"ano_aviso\" in self.loader.dataset.df.columns):\n",
    "            preprocessor.create_date(day_column=adv_day_column, month_column=adv_month_column,\n",
    "                                     year_column=adv_year_column)\n",
    "\n",
    "        if all([column in self.loader.dataset.df.columns for column in dumies_columns]):\n",
    "            preprocessor.create_dummies(dumies_columns)\n",
    "\n",
    "        preprocessor.cat_to_codes()\n",
    "        preprocessor.process_date()\n",
    "        preprocessor.process_labels()\n",
    "        preprocessor.fill_na()\n",
    "        preprocessor.drop_na()\n",
    "        preprocessor.update_dataset()\n",
    "        return preprocessor\n",
    "\n",
    "    def run_ml_preprocessing(self, dataset, validation=True, samples=[\"train\", \"validation\", \"test\"]):\n",
    "        \"\"\"\n",
    "        Performs preprocessing for machine learning on the dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset (object): The dataset object to be preprocessed.\n",
    "            validation (bool): Whether to include a validation set in the preprocessing.\n",
    "            samples (list): A list of sample names to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            object: An object that performs preprocessing for machine learning on the dataset.\n",
    "        \"\"\"\n",
    "        MLpreprocessor = self.ml_preprocessing_class(dataset)\n",
    "        MLpreprocessor.undersample()\n",
    "        MLpreprocessor.split(validation=validation)\n",
    "        MLpreprocessor.set_scaler()\n",
    "        MLpreprocessor.scale(samples)\n",
    "        return MLpreprocessor\n",
    "\n",
    "    def run_training(self, models, dataset):\n",
    "        \"\"\"\n",
    "        Trains the models.\n",
    "\n",
    "        Args:\n",
    "            models (list): A list of machine learning models to be trained.\n",
    "            dataset (object): The dataset object to be used for training.\n",
    "\n",
    "        Returns:\n",
    "            object: An object that trains the models.\n",
    "        \"\"\"\n",
    "        trainer = self.trainer_class(models, dataset)\n",
    "        trainer.train()\n",
    "        return trainer\n",
    "\n",
    "    def run_model_evaluation(self, models, dataset, sample):\n",
    "        \"\"\"\n",
    "        Evaluates the models.\n",
    "\n",
    "        Args:\n",
    "            models (list): A list of machine learning models to be evaluated.\n",
    "            dataset (object): The dataset object to be used for evaluation.\n",
    "            sample (str): The name of the sample to be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            object: An object that evaluates the models.\n",
    "        \"\"\"\n",
    "        evaluator = self.evaluation_class(models, dataset)\n",
    "        evaluator.run(sample=sample)\n",
    "        return evaluator\n",
    "\n",
    "    def run_model_selection(self, models, dataset, decision_metric=\"f1\", sample=\"validation\"):\n",
    "        \"\"\"\n",
    "        Selects the best model based on a decision metric.\n",
    "\n",
    "        Args:\n",
    "            models (list): A list of machine learning models to be evaluated and selected.\n",
    "            dataset (object): The dataset object to be used for evaluation.\n",
    "            decision_metric (str): The decision metric to be used for model selection.\n",
    "            sample (str): The name of the sample to be used for model selection.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the trainer object, evaluator object, and the best model object.\n",
    "        \"\"\"\n",
    "        trainer = self.run_training(models, dataset)\n",
    "        evaluator = self.run_model_evaluation(trainer.models, dataset, sample=sample)\n",
    "        best_model = self.model_selector_class(evaluator.models, decision_metric).select()\n",
    "        return trainer, evaluator, best_model\n",
    "\n",
    "    def run_tuning(self, model, dataset, cv=3, random_n_iter=10, grid_max_iter=10):\n",
    "        \"\"\"\n",
    "        Tunes the hyperparameters of a model.\n",
    "\n",
    "        Args:\n",
    "            model (object): The model object to be tuned.\n",
    "            dataset (object): The dataset object to be used for tuning.\n",
    "            cv (int): The number of cross-validation folds.\n",
    "            random_n_iter (int): The number of iterations for random search.\n",
    "            grid_max_iter (int): The maximum number of iterations for grid search.\n",
    "\n",
    "        Returns:\n",
    "            object: An object that tunes the hyperparameters of the model.\n",
    "        \"\"\"\n",
    "        tunning = self.tunning_class(model, dataset, cv=cv, random_state=123, scoring_fn=\"roc_auc\")\n",
    "        tunning.RandomSearch(n_iter=random_n_iter)\n",
    "        tunning.GridSearch(max_iter=grid_max_iter, amplitude=0.5, cutoff=3)\n",
    "        return tunning\n",
    "\n",
    "    def run_pipeline(self, cv, random_n_iter, grid_max_iter):\n",
    "        \"\"\"\n",
    "        Runs the entire machine learning pipeline.\n",
    "\n",
    "        Args:\n",
    "            cv (int): The number of cross-validation folds.\n",
    "            random_n_iter (int): The number of iterations for random search.\n",
    "            grid_max_iter (int): The maximum number of iterations for grid search.\n",
    "\n",
    "        Returns:\n",
    "            object: The best model object.\n",
    "        \"\"\"\n",
    "        self.preprocessor = self.run_preprocessing()\n",
    "        self.MLpreprocessor = self.run_ml_preprocessing(self.preprocessor.dataset, validation=True)\n",
    "        self.trainer, self.evaluator, best_model = self.run_model_selection(self.models, self.preprocessor.dataset)\n",
    "        self.tunning = self.run_tuning(best_model, self.MLpreprocessor.dataset, cv=cv, random_n_iter=random_n_iter,\n",
    "                                       grid_max_iter=grid_max_iter)\n",
    "        self.best_model = self.run_retrain_best_model()\n",
    "        return self.best_model.model\n",
    "\n",
    "    def run_retrain_best_model(self):\n",
    "        \"\"\"\n",
    "        Retrains the best model on the entire dataset.\n",
    "\n",
    "        Returns:\n",
    "            object: The best model object.\n",
    "        \"\"\"\n",
    "        best_model = self.tunning.best_model\n",
    "        self.bestmodel_MLpreprocessor = self.run_ml_preprocessing(self.preprocessor.dataset, validation=False,\n",
    "                                                                  samples=[\"train\", \"test\"])\n",
    "        self.bestmodel_trainer = self.run_training(best_model, self.bestmodel_MLpreprocessor.dataset)\n",
    "        self.bestmodel_evaluator = self.run_model_evaluation(best_model,\n",
    "                                                             self.bestmodel_MLpreprocessor.dataset, sample=\"test\")\n",
    "        self.pickle_model(best_model.model)\n",
    "        return best_model\n",
    "\n",
    "    def pickle_model(self, model, filename=\"best_model.pkl\"):\n",
    "        \"\"\"\n",
    "        Saves the best model to a file.\n",
    "\n",
    "        Args:\n",
    "            model (object): The model object to be saved.\n",
    "            filename (str): The filename to save the model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        pickle.dump(model, open(filename, \"wb\"))\n",
    "\n",
    "    def get_metrics(self):\n",
    "        \"\"\"\n",
    "        Returns the metrics of the best model.\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the metrics of the best model.\n",
    "        \"\"\"\n",
    "        return self.best_model.show_metrics()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        \"\"\"\n",
    "        Plots the evaluation metrics of the best model.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        return self.evaluator.plot_metrics()\n",
    "\n",
    "\n",
    "class orchestrator():\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self,models,loader,\n",
    "                 preprocessor_class:Preprocessor,\n",
    "                 ml_reprocessing_class:MLPreprocessing,\n",
    "                 trainer_class:Trainer,\n",
    "                 evaluation_class:Evaluation,\n",
    "                 model_tunning_class:ModelTunning,\n",
    "                 model_selector_class:Selector) -> None:\n",
    "        \n",
    "        \n",
    "        self.models=models\n",
    "        self.loader=loader\n",
    "        self.preprocessing_class=preprocessor_class\n",
    "        self.ml_preprocessing_class=ml_reprocessing_class\n",
    "        self.trainer_class=trainer_class\n",
    "        self.evaluation_class=evaluation_class\n",
    "        self.tunning_class=model_tunning_class\n",
    "        self.model_selector_class=model_selector_class\n",
    "\n",
    "    def run_preprocessing(self,replacement_columns=[\"tamanho_motor\",\"milhas_carro\"],replacements=[\"L\",\"mile\"],\n",
    "                          adv_day_column=\"dia_aviso\",adv_month_column=\"mes_aviso\",adv_year_column=\"ano_aviso\",dumies_columns=[\"cor\",\"tipo_cambio\"]):\n",
    "        \n",
    "        preprocessor=self.preprocessing_class(self.loader.dataset)\n",
    "        preprocessor.replacements(columns=replacement_columns,replacements=replacements)\n",
    "        preprocessor.set_types()\n",
    "\n",
    "        if (adv_day_column in self.loader.dataset.df.columns and \"mes_aviso\" in self.loader.dataset.df.columns and \"ano_aviso\" in self.loader.dataset.df.columns):                \n",
    "            preprocessor.create_date(day_column=adv_day_column,month_column=adv_month_column,year_column=adv_year_column)\n",
    "        \n",
    "        if all([column in self.loader.dataset.df.columns for column in dumies_columns]):\n",
    "            preprocessor.create_dummies(dumies_columns)     \n",
    "\n",
    "        preprocessor.cat_to_codes()\n",
    "        preprocessor.process_date()\n",
    "        preprocessor.process_labels()\n",
    "        preprocessor.fill_na()\n",
    "        preprocessor.drop_na()\n",
    "        preprocessor.update_dataset()\n",
    "        return preprocessor\n",
    "\n",
    "    def run_ml_preprocessing(self,dataset,validation=True,samples=[\"train\",\"validation\",\"test\"]):\n",
    "        MLpreprocessor=self.ml_preprocessing_class(dataset)\n",
    "        MLpreprocessor.undersample()\n",
    "        MLpreprocessor.split(validation=validation)\n",
    "        MLpreprocessor.set_scaler()\n",
    "        MLpreprocessor.scale(samples)\n",
    "        return MLpreprocessor\n",
    "\n",
    "    def run_trainnig(self,models,dataset):\n",
    "        trainer=self.trainer_class(models,dataset)\n",
    "        trainer.train()\n",
    "        return trainer\n",
    "    \n",
    "    def run_model_evaluation(self,models,dataset,sample):\n",
    "        evaluator=self.evaluation_class(models,dataset)\n",
    "        evaluator.run(sample=sample)\n",
    "        return evaluator\n",
    "    \n",
    "    def run_model_selection(self,models,dataset,decision_metric=\"f1\",sample=\"validation\"):\n",
    "        trainer=self.run_trainnig(models,dataset)\n",
    "        evaluator=self.run_model_evaluation(trainer.models,dataset,sample=sample)\n",
    "        best_model=self.model_selector_class(evaluator.models,decision_metric).select()\n",
    "        return trainer,evaluator,best_model\n",
    "\n",
    "\n",
    "    def run_tunning(self,model,dataset,cv=3,random_n_iter=10,grid_max_iter=10)->Model:\n",
    "        tunning=self.tunning_class(model,dataset,cv=cv,random_state=123,scoring_fn=\"roc_auc\")\n",
    "        tunning.RandomSearch(n_iter=random_n_iter)\n",
    "        tunning.GridSearch(max_iter=grid_max_iter,amplitude=0.5,cutoff=3)\n",
    "        return tunning\n",
    "\n",
    "    def run_pipeline(self,cv,random_n_iter,grid_max_iter):\n",
    "        self.preprocessor=self.run_preprocessing()\n",
    "        self.MLpreprocessor=self.run_ml_preprocessing(self.preprocessor.dataset,validation=True)\n",
    "        self.trainer,self.evaluator,best_model=self.run_model_selection(self.models,self.preprocessor.dataset)\n",
    "        self.tunning=self.run_tunning(best_model,self.MLpreprocessor.dataset,cv=cv,random_n_iter=random_n_iter,grid_max_iter=grid_max_iter)\n",
    "        self.best_model=self.run_retrain_best_model()\n",
    "        return self.best_model.model\n",
    "\n",
    "    \n",
    "    def run_retrain_best_model(self):\n",
    "        best_model=self.tunning.best_model\n",
    "        self.bestmodel_MLpreprocessor=self.run_ml_preprocessing(self.preprocessor.dataset,validation=False,samples=[\"train\",\"test\"])\n",
    "        self.bestmodel_trainer=self.run_trainnig(best_model,self.bestmodel_MLpreprocessor.dataset)\n",
    "        self.bestmodel_evaluator=self.run_model_evaluation(best_model,self.bestmodel_MLpreprocessor.dataset,sample=\"test\")        \n",
    "        self.pickle_model(best_model.model)\n",
    "        return best_model\n",
    "\n",
    "            \n",
    "    def pickle_model(self,model,filename=\"best_model.pkl\"):\n",
    "        pickle.dump(model,open(filename,\"wb\"))\n",
    "\n",
    "    def get_metrics(self):\n",
    "        return self.best_model.show_metrics()\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        return self.evaluator.plot_metrics()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execução do Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_map={\"Maker\":\"fabricante\",\" Genmodel\":\"modelo_carro\",\" Genmodel_ID\":\"ano_modelo_carro\",\n",
    "            \"Door_num\":\"portas\",\"Seat_num\":\"lugares\",\"repair_complexity\":\"nivel_conserto\",\n",
    "            \"repair_cost\":\"custo_conserto\",\"repair_date\":\"data_conserto\",\"repair_hours\":\"tempo_conserto\",\n",
    "            \"breakdown_date\":\"data_sinistro\",\"Fuel_type\":\"combustível\",\"Color\":\"cor\",\"Adv_year\":\"ano_aviso\",\n",
    "            \"Adv_month\":\"mes_aviso\",\"Bodytype\":\"tipo_carro\",\"issue\":\"tipo_falha\",\"issue_id\":\"categoria_falha\",\n",
    "            \"Reg_year\":\"ano_registro\",\"Engin_size\":\"tamanho_motor\",\"Gearbox\":\"tipo_cambio\",\"Adv_day\":\"dia_aviso\",\n",
    "            \"Runned_Miles\":\"milhas_carro\",\"Price\":\"preço\"}\n",
    "\n",
    "cat_columns=[\"fabricante\", 'modelo_carro', 'ano_modelo_carro', 'cor', 'tipo_carro' \n",
    "              ,'tipo_cambio', 'combustível', 'tipo_falha', 'categoria_falha','nivel_conserto']\n",
    "\n",
    "disc_columns=[\"ano_registro\",\"lugares\",\"portas\",\"dia_aviso\",'tamanho_motor','mes_aviso','ano_aviso']\n",
    "date_columns=[\"data_conserto\",\"data_sinistro\",]\n",
    "cont_columns=['preço','tempo_conserto',\"milhas_carro\",\"custo_conserto\"]\n",
    "label_columns=[\"Label\"]\n",
    "\n",
    "\n",
    "loader=Loader('vehicle_claims_labeled.csv',cat_columns=cat_columns,date_columns=date_columns,cont_columns=cont_columns,disc_columns=disc_columns,label_columns=label_columns)\n",
    "loader.rename_columns(columns_map)\n",
    "\n",
    "\n",
    "\n",
    "KNN=Model(KNeighborsClassifier,params={},supervised=True,run_scaled=False)\n",
    "KNN.grid_search_params(n_neighbors=range(3,25,2),weights=[\"uniform\",\"distance\"])\n",
    "LR=Model(LogisticRegression,params={},supervised=True,run_scaled=False)\n",
    "LR.random_grid_search_params(C=np.logspace(-4,4,20),penalty=[\"l1\",\"l2\"])\n",
    "NB=Model(GaussianNB,params={},supervised=True,run_scaled=True)\n",
    "NB.random_grid_search_params(var_smoothing=np.logspace(-9,-1,50))\n",
    "models=[KNN,LR,NB]\n",
    "ml_pipeline=orchestrator(models,loader,Preprocessor,MLPreprocessing,Trainer,Evaluation,ModelTunning,Selector)\n",
    "ml_pipeline.run_pipeline(cv=3,random_n_iter=3,grid_max_iter=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_pipeline.get_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=ml_pipeline.plot_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
